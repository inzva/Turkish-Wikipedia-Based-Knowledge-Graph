{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\r\n",
      "dev\r\n",
      "test\r\n",
      "extra\r\n"
     ]
    }
   ],
   "source": [
    "!tar -zxvf tr.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-04-08 19:06:08--  https://raw.githubusercontent.com/huggingface/transformers/master/examples/token-classification/run_ner.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 20781 (20K) [text/plain]\n",
      "Saving to: ‘run_ner.py.6’\n",
      "\n",
      "run_ner.py.6        100%[===================>]  20,29K  --.-KB/s    in 0,01s   \n",
      "\n",
      "2021-04-08 19:06:09 (1,47 MB/s) - ‘run_ner.py.6’ saved [20781/20781]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/token-classification/run_ner.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers\n",
      "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-7f573t59\n",
      "  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-7f573t59\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied (use --upgrade to upgrade): transformers==4.6.0.dev0 from git+https://github.com/huggingface/transformers in /home/ociftci/anaconda3/lib/python3.7/site-packages\n",
      "Requirement already satisfied: sacremoses in /home/ociftci/anaconda3/lib/python3.7/site-packages (from transformers==4.6.0.dev0) (0.0.43)\n",
      "Requirement already satisfied: requests in /home/ociftci/anaconda3/lib/python3.7/site-packages (from transformers==4.6.0.dev0) (2.22.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ociftci/anaconda3/lib/python3.7/site-packages (from transformers==4.6.0.dev0) (1.18.1)\n",
      "Requirement already satisfied: filelock in /home/ociftci/anaconda3/lib/python3.7/site-packages (from transformers==4.6.0.dev0) (3.0.12)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/ociftci/anaconda3/lib/python3.7/site-packages (from transformers==4.6.0.dev0) (0.10.1)\n",
      "Requirement already satisfied: packaging in /home/ociftci/anaconda3/lib/python3.7/site-packages (from transformers==4.6.0.dev0) (20.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ociftci/anaconda3/lib/python3.7/site-packages (from transformers==4.6.0.dev0) (2020.10.15)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/ociftci/anaconda3/lib/python3.7/site-packages (from transformers==4.6.0.dev0) (3.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ociftci/anaconda3/lib/python3.7/site-packages (from transformers==4.6.0.dev0) (4.42.1)\n",
      "Requirement already satisfied: joblib in /home/ociftci/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers==4.6.0.dev0) (0.14.1)\n",
      "Requirement already satisfied: click in /home/ociftci/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers==4.6.0.dev0) (7.0)\n",
      "Requirement already satisfied: six in /home/ociftci/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers==4.6.0.dev0) (1.14.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/ociftci/anaconda3/lib/python3.7/site-packages (from requests->transformers==4.6.0.dev0) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ociftci/anaconda3/lib/python3.7/site-packages (from requests->transformers==4.6.0.dev0) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ociftci/anaconda3/lib/python3.7/site-packages (from requests->transformers==4.6.0.dev0) (1.25.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ociftci/anaconda3/lib/python3.7/site-packages (from requests->transformers==4.6.0.dev0) (3.0.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ociftci/anaconda3/lib/python3.7/site-packages (from packaging->transformers==4.6.0.dev0) (2.4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ociftci/anaconda3/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.6.0.dev0) (2.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /home/ociftci/anaconda3/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.6.0.dev0) (3.7.4.3)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.6.0.dev0-py3-none-any.whl size=2065210 sha256=e305efdcd583a6196042102f2a4aa95bfc409f718be8941c858b525b34d77cd4\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ek64zopi/wheels/35/2e/a7/d819e3310040329f0f47e57c9e3e7a7338aa5e74c49acfe522\n",
      "Successfully built transformers\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/ociftci/anaconda3/lib/python3.7/site-packages (1.1.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ociftci/anaconda3/lib/python3.7/site-packages (from datasets) (1.18.1)\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /home/ociftci/anaconda3/lib/python3.7/site-packages (from datasets) (2.0.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/ociftci/anaconda3/lib/python3.7/site-packages (from datasets) (2.22.0)\n",
      "Requirement already satisfied: pandas in /home/ociftci/anaconda3/lib/python3.7/site-packages (from datasets) (1.0.1)\n",
      "Requirement already satisfied: xxhash in /home/ociftci/anaconda3/lib/python3.7/site-packages (from datasets) (2.0.0)\n",
      "Requirement already satisfied: filelock in /home/ociftci/anaconda3/lib/python3.7/site-packages (from datasets) (3.0.12)\n",
      "Requirement already satisfied: multiprocess in /home/ociftci/anaconda3/lib/python3.7/site-packages (from datasets) (0.70.11.1)\n",
      "Requirement already satisfied: dill in /home/ociftci/anaconda3/lib/python3.7/site-packages (from datasets) (0.3.3)\n",
      "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /home/ociftci/anaconda3/lib/python3.7/site-packages (from datasets) (4.42.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ociftci/anaconda3/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.25.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ociftci/anaconda3/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ociftci/anaconda3/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/ociftci/anaconda3/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.8)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ociftci/anaconda3/lib/python3.7/site-packages (from pandas->datasets) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/ociftci/anaconda3/lib/python3.7/site-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ociftci/anaconda3/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas->datasets) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_csv(filename):\n",
    "  with open(filename, \"r\") as f:\n",
    "   data = f.readlines()\n",
    "\n",
    "  for item in data[0:40]:\n",
    "   print(item.split())\n",
    "  \n",
    "  \n",
    "  with open(filename + '.json', 'w', encoding='utf-8') as csvfile:\n",
    "    words = []\n",
    "    ner = []\n",
    "    for item in data:\n",
    "        \n",
    "      item = item.split()\n",
    "      if not item:\n",
    "          d = {}\n",
    "          d[\"words\"] = words\n",
    "          d[\"ner\"] = ner\n",
    "          csvfile.write(json.dumps(d, ensure_ascii=False) + \"\\n\")\n",
    "          words = []\n",
    "          ner = []\n",
    "      else:\n",
    "        words.append(item[0][3:])\n",
    "        ner.append(item[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=0,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tr:3.lük', 'O']\n",
      "['tr:maçında', 'O']\n",
      "['tr:Slovenya', 'B-ORG']\n",
      "['tr:Millî', 'I-ORG']\n",
      "['tr:Basketbol', 'I-ORG']\n",
      "[\"tr:Takımı'nı\", 'I-ORG']\n",
      "['tr:yendikleri', 'O']\n",
      "['tr:maçta', 'O']\n",
      "['tr:23', 'O']\n",
      "['tr:sayı', 'O']\n",
      "['tr:,', 'O']\n",
      "['tr:6', 'O']\n",
      "['tr:ribaund', 'O']\n",
      "['tr:,', 'O']\n",
      "['tr:2', 'O']\n",
      "['tr:blok', 'O']\n",
      "['tr:istatistikleriyle', 'O']\n",
      "['tr:oynamış', 'O']\n",
      "['tr:ve', 'O']\n",
      "['tr:12', 'O']\n",
      "['tr:faul', 'O']\n",
      "['tr:yaptırmıştır', 'O']\n",
      "['tr:.', 'O']\n",
      "[]\n",
      "[\"tr:'\", 'O']\n",
      "[\"tr:''\", 'O']\n",
      "['tr:Denizlispor', 'B-ORG']\n",
      "[\"tr:''\", 'O']\n",
      "[\"tr:'\", 'O']\n",
      "[]\n",
      "['tr:Hami', 'B-PER']\n",
      "['tr:Mandıralı', 'I-PER']\n",
      "['tr:36', 'O']\n",
      "['tr:,', 'O']\n",
      "['tr:Orhan', 'B-PER']\n",
      "['tr:Çıkırıkçı', 'I-PER']\n",
      "['tr:46', 'O']\n",
      "['tr:,', 'O']\n",
      "['tr:48', 'O']\n",
      "['tr:,', 'O']\n",
      "['tr:Pro', 'B-ORG']\n",
      "['tr:League', 'I-ORG']\n",
      "['tr:ekiplerinden', 'O']\n",
      "['tr:Club', 'B-ORG']\n",
      "[\"tr:Brugge'de\", 'I-ORG']\n",
      "['tr:oynuyor', 'O']\n",
      "['tr:.', 'O']\n",
      "[]\n",
      "['tr:Eğitimini', 'O']\n",
      "['tr:o', 'O']\n",
      "['tr:kentte', 'O']\n",
      "['tr:bitirdikten', 'O']\n",
      "['tr:sonra', 'O']\n",
      "['tr:,', 'O']\n",
      "['tr:Bağdat', 'B-LOC']\n",
      "['tr:Nizamiye', 'B-ORG']\n",
      "[\"tr:Medresesi'ne\", 'I-ORG']\n",
      "['tr:devam', 'O']\n",
      "['tr:etti', 'O']\n",
      "['tr:.', 'O']\n",
      "[]\n",
      "['tr:YÖNLENDİR', 'O']\n",
      "['tr:Efemçukuru', 'B-LOC']\n",
      "['tr:,', 'I-LOC']\n",
      "['tr:Menderes', 'I-LOC']\n",
      "[]\n",
      "['tr:Daha', 'O']\n",
      "['tr:sonra', 'O']\n",
      "['tr:Cumhurbaşkanı', 'O']\n",
      "['tr:Michel', 'B-PER']\n",
      "['tr:Djotodia', 'I-PER']\n",
      "['tr:kabinesinde', 'O']\n",
      "['tr:görev', 'O']\n",
      "['tr:yaptı', 'O']\n",
      "['tr:ve', 'O']\n",
      "['tr:Başbakan', 'O']\n",
      "['tr:olarak', 'O']\n",
      "['tr:atanmadan', 'O']\n",
      "['tr:önce', 'O']\n",
      "['tr:geçici', 'O']\n",
      "['tr:Yıldız', 'B-ORG']\n",
      "['tr:Savaşları', 'I-ORG']\n",
      "['tr::', 'I-ORG']\n",
      "['tr:Bölüm', 'I-ORG']\n",
      "['tr:II', 'I-ORG']\n",
      "['tr:-', 'I-ORG']\n",
      "['tr:Klonların', 'I-ORG']\n",
      "['tr:Saldırısı', 'I-ORG']\n",
      "[\"tr:''\", 'O']\n",
      "[]\n",
      "['tr:1998-2004', 'O']\n",
      "['tr::', 'O']\n",
      "['tr:Kombassan', 'B-ORG']\n",
      "['tr:Holding', 'I-ORG']\n",
      "[]\n",
      "[\"tr:Avustralya'da\", 'B-LOC']\n",
      "['tr:25', 'O']\n",
      "['tr:numaraya', 'O']\n",
      "['tr:çıkmış', 'O']\n",
      "['tr:,', 'O']\n",
      "['tr:ayrıca', 'O']\n",
      "['tr:Yeni', 'B-LOC']\n",
      "['tr:Zelanda', 'I-LOC']\n",
      "['tr:listesine', 'O']\n",
      "['tr:32', 'O']\n",
      "['tr:numaradan', 'O']\n",
      "['tr:giriş', 'O']\n",
      "['tr:yapmış', 'O']\n",
      "['tr:ve', 'O']\n",
      "['tr:8', 'O']\n",
      "['tr:numaraya', 'O']\n",
      "['tr:çıkmıştır', 'O']\n",
      "['tr:.', 'O']\n",
      "[]\n",
      "['tr:Piet', 'B-PER']\n",
      "['tr:Mondrian', 'I-PER']\n",
      "['tr:(', 'O']\n",
      "['tr:1872-1944', 'O']\n",
      "['tr:)', 'O']\n",
      "[]\n",
      "[\"tr:'\", 'O']\n",
      "[\"tr:''\", 'O']\n",
      "['tr:Ruth', 'B-PER']\n",
      "['tr:Gordon', 'I-PER']\n",
      "['tr:†', 'O']\n",
      "[]\n",
      "['tr:Opel', 'B-ORG']\n",
      "['tr:(', 'O']\n",
      "['tr:General', 'B-ORG']\n",
      "[\"tr:Motors'a\", 'I-ORG']\n",
      "['tr:ait', 'O']\n",
      "['tr:)', 'O']\n",
      "[]\n",
      "['tr:China', 'B-PER']\n",
      "['tr:Forbes', 'I-PER']\n",
      "['tr:—', 'O']\n",
      "['tr:vokal', 'O']\n",
      "[]\n",
      "['tr:Nokia', 'B-LOC']\n",
      "['tr:Arena', 'I-LOC']\n",
      "['tr:(', 'O']\n",
      "['tr:11,700', 'O']\n",
      "['tr:)', 'O']\n",
      "[]\n",
      "['tr:YÖNLENDİRME', 'O']\n",
      "['tr:Rakka', 'B-ORG']\n",
      "['tr:(', 'I-ORG']\n",
      "['tr:il', 'I-ORG']\n",
      "['tr:)', 'I-ORG']\n",
      "[]\n",
      "[\"tr:Etiyopya'da\", 'B-LOC']\n",
      "['tr:ki', 'O']\n",
      "['tr:görevi', 'O']\n",
      "['tr:sırasında', 'O']\n",
      "['tr:girdikleri', 'O']\n",
      "['tr:çatışmada', 'O']\n",
      "['tr:arkadaşı', 'O']\n",
      "[\"tr:Donnie'yi\", 'O']\n",
      "['tr:kaybeder', 'O']\n",
      "['tr:ve', 'O']\n"
     ]
    }
   ],
   "source": [
    "convert_to_csv(\"train\")\n",
    "convert_to_csv(\"test\")\n",
    "convert_to_csv(\"dev\")\n",
    "convert_to_csv(\"extra\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat train.json extra.json > all_train.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122836\r\n"
     ]
    }
   ],
   "source": [
    "!cat all_train.json | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seqeval in /home/ociftci/anaconda3/lib/python3.7/site-packages (0.0.19)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /home/ociftci/anaconda3/lib/python3.7/site-packages (from seqeval) (1.18.1)\n",
      "Requirement already satisfied: Keras>=2.2.4 in /home/ociftci/anaconda3/lib/python3.7/site-packages (from seqeval) (2.4.3)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/ociftci/anaconda3/lib/python3.7/site-packages (from Keras>=2.2.4->seqeval) (1.4.1)\n",
      "Requirement already satisfied: pyyaml in /home/ociftci/anaconda3/lib/python3.7/site-packages (from Keras>=2.2.4->seqeval) (5.3)\n",
      "Requirement already satisfied: h5py in /home/ociftci/anaconda3/lib/python3.7/site-packages (from Keras>=2.2.4->seqeval) (2.10.0)\n",
      "Requirement already satisfied: six in /home/ociftci/anaconda3/lib/python3.7/site-packages (from h5py->Keras>=2.2.4->seqeval) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/08/2021 19:13:21 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 2distributed training: False, 16-bits training: False\n",
      "04/08/2021 19:13:21 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=./test-convbert-ner, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Apr08_19-13-21_inzva-GPU, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=./test-convbert-ner, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, _n_gpu=2, mp_parameters=)\n",
      "Using custom data configuration default\n",
      "Reusing dataset json (/home/ociftci/.cache/huggingface/datasets/json/default-ce856146dcc63496/0.0.0/fb88b12bd94767cb0cc7eedcd82ea1f402d2162addc03a37e81d4f8dc7313ad9)\n",
      "[INFO|configuration_utils.py:490] 2021-04-08 19:13:26,300 >> loading configuration file https://huggingface.co/dbmdz/convbert-base-turkish-cased/resolve/main/config.json from cache at /home/ociftci/.cache/huggingface/transformers/739a3782496547a38ea8d520ea5a1172d30792ed221dbda8c0aa1ff6f7123de5.46675d4724e3502b29063c42e6bf803b2f69272b89c8645ccc825a7be6b4cc77\n",
      "[INFO|configuration_utils.py:526] 2021-04-08 19:13:26,301 >> Model config ConvBertConfig {\n",
      "  \"architectures\": [\n",
      "    \"ConvBertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"conv_kernel_size\": 9,\n",
      "  \"embedding_size\": 768,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": \"ner\",\n",
      "  \"head_ratio\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"convbert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.6.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:490] 2021-04-08 19:13:26,769 >> loading configuration file https://huggingface.co/dbmdz/convbert-base-turkish-cased/resolve/main/config.json from cache at /home/ociftci/.cache/huggingface/transformers/739a3782496547a38ea8d520ea5a1172d30792ed221dbda8c0aa1ff6f7123de5.46675d4724e3502b29063c42e6bf803b2f69272b89c8645ccc825a7be6b4cc77\n",
      "[INFO|configuration_utils.py:526] 2021-04-08 19:13:26,770 >> Model config ConvBertConfig {\n",
      "  \"architectures\": [\n",
      "    \"ConvBertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"conv_kernel_size\": 9,\n",
      "  \"embedding_size\": 768,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_ratio\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"convbert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.6.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1713] 2021-04-08 19:13:29,188 >> loading file https://huggingface.co/dbmdz/convbert-base-turkish-cased/resolve/main/vocab.txt from cache at /home/ociftci/.cache/huggingface/transformers/a07ea7d27a9985bc8970f437ecc776a7b9f157d61505a4eb7a8a1d73afee806b.2439760968a7b05b87c85c85cd02e8aba6e01e526263c6be3a99b2d64c4cee1d\n",
      "[INFO|tokenization_utils_base.py:1713] 2021-04-08 19:13:29,191 >> loading file https://huggingface.co/dbmdz/convbert-base-turkish-cased/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1713] 2021-04-08 19:13:29,191 >> loading file https://huggingface.co/dbmdz/convbert-base-turkish-cased/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1713] 2021-04-08 19:13:29,192 >> loading file https://huggingface.co/dbmdz/convbert-base-turkish-cased/resolve/main/tokenizer_config.json from cache at /home/ociftci/.cache/huggingface/transformers/defae037eb43e42d1ff321448911d886c1b2330b597922e9812c4624b37bed58.bd85f6a0d9c2848260fdda7056a557a7fa213a57b5ed27b530abd20572c0cbf4\n",
      "[INFO|tokenization_utils_base.py:1713] 2021-04-08 19:13:29,192 >> loading file https://huggingface.co/dbmdz/convbert-base-turkish-cased/resolve/main/tokenizer.json from cache at None\n",
      "[INFO|modeling_utils.py:1052] 2021-04-08 19:13:29,807 >> loading weights file https://huggingface.co/dbmdz/convbert-base-turkish-cased/resolve/main/pytorch_model.bin from cache at /home/ociftci/.cache/huggingface/transformers/1b18f145f2036d55f5c9b25886cf9f2ef7996fd9dea5cd82bb2dc7cbcb35115e.485dbf02ec4fdefb5958b82cfd2d2a91445a7391db6bd66d295bbee03aaa0f35\n",
      "[INFO|modeling_utils.py:1168] 2021-04-08 19:13:35,876 >> All model checkpoint weights were used when initializing ConvBertForTokenClassification.\n",
      "\n",
      "[WARNING|modeling_utils.py:1171] 2021-04-08 19:13:35,877 >> Some weights of ConvBertForTokenClassification were not initialized from the model checkpoint at dbmdz/convbert-base-turkish-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|█████████████████████████████████████████| 123/123 [00:16<00:00,  7.35ba/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:01<00:00,  7.17ba/s]\n",
      "[INFO|trainer.py:491] 2021-04-08 19:14:03,249 >> The following columns in the training set  don't have a corresponding argument in `ConvBertForTokenClassification.forward` and have been ignored: ner, words.\n",
      "[INFO|trainer.py:491] 2021-04-08 19:14:03,249 >> The following columns in the evaluation set  don't have a corresponding argument in `ConvBertForTokenClassification.forward` and have been ignored: ner, words.\n",
      "[INFO|trainer.py:1013] 2021-04-08 19:14:03,819 >> ***** Running training *****\n",
      "[INFO|trainer.py:1014] 2021-04-08 19:14:03,819 >>   Num examples = 122836\n",
      "[INFO|trainer.py:1015] 2021-04-08 19:14:03,819 >>   Num Epochs = 5\n",
      "[INFO|trainer.py:1016] 2021-04-08 19:14:03,819 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1017] 2021-04-08 19:14:03,819 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "[INFO|trainer.py:1018] 2021-04-08 19:14:03,819 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1019] 2021-04-08 19:14:03,819 >>   Total optimization steps = 9600\n",
      "  0%|                                                  | 0/9600 [00:00<?, ?it/s]/home/ociftci/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 0.2081, 'learning_rate': 4.739583333333333e-05, 'epoch': 0.26}         \n",
      "  5%|█▉                                    | 500/9600 [05:41<1:39:51,  1.52it/s][INFO|trainer.py:1648] 2021-04-08 19:19:44,933 >> Saving model checkpoint to ./test-convbert-ner/checkpoint-500\n",
      "[INFO|configuration_utils.py:329] 2021-04-08 19:19:44,935 >> Configuration saved in ./test-convbert-ner/checkpoint-500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:831] 2021-04-08 19:19:46,795 >> Model weights saved in ./test-convbert-ner/checkpoint-500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2021-04-08 19:19:46,796 >> tokenizer config file saved in ./test-convbert-ner/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2021-04-08 19:19:46,797 >> Special tokens file saved in ./test-convbert-ner/checkpoint-500/special_tokens_map.json\n",
      "/home/ociftci/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 0.1042, 'learning_rate': 4.4791666666666673e-05, 'epoch': 0.52}        \n",
      " 10%|███▊                                 | 1000/9600 [11:20<1:36:08,  1.49it/s][INFO|trainer.py:1648] 2021-04-08 19:25:24,185 >> Saving model checkpoint to ./test-convbert-ner/checkpoint-1000\n",
      "[INFO|configuration_utils.py:329] 2021-04-08 19:25:24,191 >> Configuration saved in ./test-convbert-ner/checkpoint-1000/config.json\n",
      "[INFO|modeling_utils.py:831] 2021-04-08 19:25:26,118 >> Model weights saved in ./test-convbert-ner/checkpoint-1000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2021-04-08 19:25:26,119 >> tokenizer config file saved in ./test-convbert-ner/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2021-04-08 19:25:26,119 >> Special tokens file saved in ./test-convbert-ner/checkpoint-1000/special_tokens_map.json\n",
      "/home/ociftci/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 0.0927, 'learning_rate': 4.21875e-05, 'epoch': 0.78}                   \n",
      " 16%|█████▊                               | 1500/9600 [16:58<1:30:28,  1.49it/s][INFO|trainer.py:1648] 2021-04-08 19:31:02,812 >> Saving model checkpoint to ./test-convbert-ner/checkpoint-1500\n",
      "[INFO|configuration_utils.py:329] 2021-04-08 19:31:02,816 >> Configuration saved in ./test-convbert-ner/checkpoint-1500/config.json\n",
      "[INFO|modeling_utils.py:831] 2021-04-08 19:31:04,658 >> Model weights saved in ./test-convbert-ner/checkpoint-1500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2021-04-08 19:31:04,659 >> tokenizer config file saved in ./test-convbert-ner/checkpoint-1500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2021-04-08 19:31:04,659 >> Special tokens file saved in ./test-convbert-ner/checkpoint-1500/special_tokens_map.json\n",
      "/home/ociftci/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 0.0782, 'learning_rate': 3.958333333333333e-05, 'epoch': 1.04}         \n",
      " 21%|███████▋                             | 2000/9600 [22:36<1:23:34,  1.52it/s][INFO|trainer.py:1648] 2021-04-08 19:36:40,274 >> Saving model checkpoint to ./test-convbert-ner/checkpoint-2000\n",
      "[INFO|configuration_utils.py:329] 2021-04-08 19:36:40,277 >> Configuration saved in ./test-convbert-ner/checkpoint-2000/config.json\n",
      "[INFO|modeling_utils.py:831] 2021-04-08 19:36:42,231 >> Model weights saved in ./test-convbert-ner/checkpoint-2000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2021-04-08 19:36:42,232 >> tokenizer config file saved in ./test-convbert-ner/checkpoint-2000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2021-04-08 19:36:42,232 >> Special tokens file saved in ./test-convbert-ner/checkpoint-2000/special_tokens_map.json\n",
      "/home/ociftci/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 0.0551, 'learning_rate': 3.697916666666667e-05, 'epoch': 1.3}          \n",
      " 26%|█████████▋                           | 2500/9600 [28:12<1:18:18,  1.51it/s][INFO|trainer.py:1648] 2021-04-08 19:42:15,968 >> Saving model checkpoint to ./test-convbert-ner/checkpoint-2500\n",
      "[INFO|configuration_utils.py:329] 2021-04-08 19:42:15,975 >> Configuration saved in ./test-convbert-ner/checkpoint-2500/config.json\n",
      "[INFO|modeling_utils.py:831] 2021-04-08 19:42:17,944 >> Model weights saved in ./test-convbert-ner/checkpoint-2500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2021-04-08 19:42:17,945 >> tokenizer config file saved in ./test-convbert-ner/checkpoint-2500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2021-04-08 19:42:17,945 >> Special tokens file saved in ./test-convbert-ner/checkpoint-2500/special_tokens_map.json\n",
      "/home/ociftci/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 0.0522, 'learning_rate': 3.4375e-05, 'epoch': 1.56}                    \n",
      " 31%|███████████▌                         | 3000/9600 [33:47<1:11:18,  1.54it/s][INFO|trainer.py:1648] 2021-04-08 19:47:51,460 >> Saving model checkpoint to ./test-convbert-ner/checkpoint-3000\n",
      "[INFO|configuration_utils.py:329] 2021-04-08 19:47:51,464 >> Configuration saved in ./test-convbert-ner/checkpoint-3000/config.json\n",
      "[INFO|modeling_utils.py:831] 2021-04-08 19:47:53,179 >> Model weights saved in ./test-convbert-ner/checkpoint-3000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2021-04-08 19:47:53,180 >> tokenizer config file saved in ./test-convbert-ner/checkpoint-3000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2021-04-08 19:47:53,180 >> Special tokens file saved in ./test-convbert-ner/checkpoint-3000/special_tokens_map.json\n",
      "/home/ociftci/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 0.049, 'learning_rate': 3.177083333333333e-05, 'epoch': 1.82}          \n",
      " 36%|█████████████▍                       | 3500/9600 [39:23<1:07:47,  1.50it/s][INFO|trainer.py:1648] 2021-04-08 19:53:27,072 >> Saving model checkpoint to ./test-convbert-ner/checkpoint-3500\n",
      "[INFO|configuration_utils.py:329] 2021-04-08 19:53:27,074 >> Configuration saved in ./test-convbert-ner/checkpoint-3500/config.json\n",
      "[INFO|modeling_utils.py:831] 2021-04-08 19:53:28,852 >> Model weights saved in ./test-convbert-ner/checkpoint-3500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2021-04-08 19:53:28,853 >> tokenizer config file saved in ./test-convbert-ner/checkpoint-3500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2021-04-08 19:53:28,854 >> Special tokens file saved in ./test-convbert-ner/checkpoint-3500/special_tokens_map.json\n",
      "/home/ociftci/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 0.0425, 'learning_rate': 2.916666666666667e-05, 'epoch': 2.08}         \n",
      " 42%|███████████████▍                     | 4000/9600 [44:57<1:00:03,  1.55it/s][INFO|trainer.py:1648] 2021-04-08 19:59:00,981 >> Saving model checkpoint to ./test-convbert-ner/checkpoint-4000\n",
      "[INFO|configuration_utils.py:329] 2021-04-08 19:59:00,983 >> Configuration saved in ./test-convbert-ner/checkpoint-4000/config.json\n",
      "[INFO|modeling_utils.py:831] 2021-04-08 19:59:02,923 >> Model weights saved in ./test-convbert-ner/checkpoint-4000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2021-04-08 19:59:02,924 >> tokenizer config file saved in ./test-convbert-ner/checkpoint-4000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2021-04-08 19:59:02,924 >> Special tokens file saved in ./test-convbert-ner/checkpoint-4000/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ociftci/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 0.0283, 'learning_rate': 2.6562500000000002e-05, 'epoch': 2.34}        \n",
      " 47%|██████████████████▎                    | 4500/9600 [50:31<55:57,  1.52it/s][INFO|trainer.py:1648] 2021-04-08 20:04:35,202 >> Saving model checkpoint to ./test-convbert-ner/checkpoint-4500\n",
      "[INFO|configuration_utils.py:329] 2021-04-08 20:04:35,205 >> Configuration saved in ./test-convbert-ner/checkpoint-4500/config.json\n",
      "[INFO|modeling_utils.py:831] 2021-04-08 20:04:37,130 >> Model weights saved in ./test-convbert-ner/checkpoint-4500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2021-04-08 20:04:37,131 >> tokenizer config file saved in ./test-convbert-ner/checkpoint-4500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2021-04-08 20:04:37,131 >> Special tokens file saved in ./test-convbert-ner/checkpoint-4500/special_tokens_map.json\n",
      "/home/ociftci/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 0.0271, 'learning_rate': 2.3958333333333334e-05, 'epoch': 2.6}         \n",
      " 52%|████████████████████▎                  | 5000/9600 [56:05<48:00,  1.60it/s][INFO|trainer.py:1648] 2021-04-08 20:10:09,615 >> Saving model checkpoint to ./test-convbert-ner/checkpoint-5000\n",
      "[INFO|configuration_utils.py:329] 2021-04-08 20:10:09,618 >> Configuration saved in ./test-convbert-ner/checkpoint-5000/config.json\n",
      "[INFO|modeling_utils.py:831] 2021-04-08 20:10:11,535 >> Model weights saved in ./test-convbert-ner/checkpoint-5000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2021-04-08 20:10:11,537 >> tokenizer config file saved in ./test-convbert-ner/checkpoint-5000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2021-04-08 20:10:11,537 >> Special tokens file saved in ./test-convbert-ner/checkpoint-5000/special_tokens_map.json\n",
      "/home/ociftci/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 0.0281, 'learning_rate': 2.1354166666666666e-05, 'epoch': 2.86}        \n",
      " 57%|█████████████████████▏               | 5500/9600 [1:01:38<44:37,  1.53it/s][INFO|trainer.py:1648] 2021-04-08 20:15:42,361 >> Saving model checkpoint to ./test-convbert-ner/checkpoint-5500\n",
      "[INFO|configuration_utils.py:329] 2021-04-08 20:15:42,365 >> Configuration saved in ./test-convbert-ner/checkpoint-5500/config.json\n",
      "[INFO|modeling_utils.py:831] 2021-04-08 20:15:44,219 >> Model weights saved in ./test-convbert-ner/checkpoint-5500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2021-04-08 20:15:44,221 >> tokenizer config file saved in ./test-convbert-ner/checkpoint-5500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2021-04-08 20:15:44,221 >> Special tokens file saved in ./test-convbert-ner/checkpoint-5500/special_tokens_map.json\n",
      "/home/ociftci/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 0.0203, 'learning_rate': 1.8750000000000002e-05, 'epoch': 3.12}        \n",
      " 62%|███████████████████████▏             | 6000/9600 [1:07:12<39:15,  1.53it/s][INFO|trainer.py:1648] 2021-04-08 20:21:16,185 >> Saving model checkpoint to ./test-convbert-ner/checkpoint-6000\n",
      "[INFO|configuration_utils.py:329] 2021-04-08 20:21:16,186 >> Configuration saved in ./test-convbert-ner/checkpoint-6000/config.json\n",
      "[INFO|modeling_utils.py:831] 2021-04-08 20:21:18,073 >> Model weights saved in ./test-convbert-ner/checkpoint-6000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2021-04-08 20:21:18,075 >> tokenizer config file saved in ./test-convbert-ner/checkpoint-6000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2021-04-08 20:21:18,075 >> Special tokens file saved in ./test-convbert-ner/checkpoint-6000/special_tokens_map.json\n",
      "/home/ociftci/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 0.0155, 'learning_rate': 1.6145833333333334e-05, 'epoch': 3.39}        \n",
      " 68%|█████████████████████████            | 6500/9600 [1:12:45<33:15,  1.55it/s][INFO|trainer.py:1648] 2021-04-08 20:26:48,932 >> Saving model checkpoint to ./test-convbert-ner/checkpoint-6500\n",
      "[INFO|configuration_utils.py:329] 2021-04-08 20:26:48,935 >> Configuration saved in ./test-convbert-ner/checkpoint-6500/config.json\n",
      "[INFO|modeling_utils.py:831] 2021-04-08 20:26:50,773 >> Model weights saved in ./test-convbert-ner/checkpoint-6500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2021-04-08 20:26:50,775 >> tokenizer config file saved in ./test-convbert-ner/checkpoint-6500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2021-04-08 20:26:50,776 >> Special tokens file saved in ./test-convbert-ner/checkpoint-6500/special_tokens_map.json\n",
      "/home/ociftci/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 0.0161, 'learning_rate': 1.3541666666666666e-05, 'epoch': 3.65}        \n",
      " 73%|██████████████████████████▉          | 7000/9600 [1:18:18<28:35,  1.52it/s][INFO|trainer.py:1648] 2021-04-08 20:32:22,344 >> Saving model checkpoint to ./test-convbert-ner/checkpoint-7000\n",
      "[INFO|configuration_utils.py:329] 2021-04-08 20:32:22,347 >> Configuration saved in ./test-convbert-ner/checkpoint-7000/config.json\n",
      "[INFO|modeling_utils.py:831] 2021-04-08 20:32:24,218 >> Model weights saved in ./test-convbert-ner/checkpoint-7000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2021-04-08 20:32:24,219 >> tokenizer config file saved in ./test-convbert-ner/checkpoint-7000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2021-04-08 20:32:24,220 >> Special tokens file saved in ./test-convbert-ner/checkpoint-7000/special_tokens_map.json\n",
      "/home/ociftci/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 0.0123, 'learning_rate': 1.09375e-05, 'epoch': 3.91}                   \n",
      " 78%|████████████████████████████▉        | 7500/9600 [1:23:50<23:18,  1.50it/s][INFO|trainer.py:1648] 2021-04-08 20:37:54,040 >> Saving model checkpoint to ./test-convbert-ner/checkpoint-7500\n",
      "[INFO|configuration_utils.py:329] 2021-04-08 20:37:54,041 >> Configuration saved in ./test-convbert-ner/checkpoint-7500/config.json\n",
      "[INFO|modeling_utils.py:831] 2021-04-08 20:37:55,898 >> Model weights saved in ./test-convbert-ner/checkpoint-7500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2021-04-08 20:37:55,899 >> tokenizer config file saved in ./test-convbert-ner/checkpoint-7500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2021-04-08 20:37:55,899 >> Special tokens file saved in ./test-convbert-ner/checkpoint-7500/special_tokens_map.json\n",
      "/home/ociftci/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0098, 'learning_rate': 8.333333333333334e-06, 'epoch': 4.17}         \n",
      " 83%|██████████████████████████████▊      | 8000/9600 [1:29:22<17:25,  1.53it/s][INFO|trainer.py:1648] 2021-04-08 20:43:26,582 >> Saving model checkpoint to ./test-convbert-ner/checkpoint-8000\n",
      "[INFO|configuration_utils.py:329] 2021-04-08 20:43:26,585 >> Configuration saved in ./test-convbert-ner/checkpoint-8000/config.json\n",
      "[INFO|modeling_utils.py:831] 2021-04-08 20:43:28,553 >> Model weights saved in ./test-convbert-ner/checkpoint-8000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2021-04-08 20:43:28,557 >> tokenizer config file saved in ./test-convbert-ner/checkpoint-8000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2021-04-08 20:43:28,559 >> Special tokens file saved in ./test-convbert-ner/checkpoint-8000/special_tokens_map.json\n",
      "/home/ociftci/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 0.0088, 'learning_rate': 5.729166666666667e-06, 'epoch': 4.43}         \n",
      " 89%|████████████████████████████████▊    | 8500/9600 [1:34:55<11:48,  1.55it/s][INFO|trainer.py:1648] 2021-04-08 20:48:58,920 >> Saving model checkpoint to ./test-convbert-ner/checkpoint-8500\n",
      "[INFO|configuration_utils.py:329] 2021-04-08 20:48:58,928 >> Configuration saved in ./test-convbert-ner/checkpoint-8500/config.json\n",
      "[INFO|modeling_utils.py:831] 2021-04-08 20:49:00,871 >> Model weights saved in ./test-convbert-ner/checkpoint-8500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2021-04-08 20:49:00,874 >> tokenizer config file saved in ./test-convbert-ner/checkpoint-8500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2021-04-08 20:49:00,875 >> Special tokens file saved in ./test-convbert-ner/checkpoint-8500/special_tokens_map.json\n",
      "/home/ociftci/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 0.0079, 'learning_rate': 3.125e-06, 'epoch': 4.69}                     \n",
      " 94%|██████████████████████████████████▋  | 9000/9600 [1:40:25<06:16,  1.59it/s][INFO|trainer.py:1648] 2021-04-08 20:54:29,339 >> Saving model checkpoint to ./test-convbert-ner/checkpoint-9000\n",
      "[INFO|configuration_utils.py:329] 2021-04-08 20:54:29,341 >> Configuration saved in ./test-convbert-ner/checkpoint-9000/config.json\n",
      "[INFO|modeling_utils.py:831] 2021-04-08 20:54:31,290 >> Model weights saved in ./test-convbert-ner/checkpoint-9000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2021-04-08 20:54:31,291 >> tokenizer config file saved in ./test-convbert-ner/checkpoint-9000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2021-04-08 20:54:31,292 >> Special tokens file saved in ./test-convbert-ner/checkpoint-9000/special_tokens_map.json\n",
      "/home/ociftci/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 0.0072, 'learning_rate': 5.208333333333334e-07, 'epoch': 4.95}         \n",
      " 99%|████████████████████████████████████▌| 9500/9600 [1:45:57<01:04,  1.55it/s][INFO|trainer.py:1648] 2021-04-08 21:00:01,033 >> Saving model checkpoint to ./test-convbert-ner/checkpoint-9500\n",
      "[INFO|configuration_utils.py:329] 2021-04-08 21:00:01,036 >> Configuration saved in ./test-convbert-ner/checkpoint-9500/config.json\n",
      "[INFO|modeling_utils.py:831] 2021-04-08 21:00:02,972 >> Model weights saved in ./test-convbert-ner/checkpoint-9500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2021-04-08 21:00:02,974 >> tokenizer config file saved in ./test-convbert-ner/checkpoint-9500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2021-04-08 21:00:02,974 >> Special tokens file saved in ./test-convbert-ner/checkpoint-9500/special_tokens_map.json\n",
      "/home/ociftci/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "100%|█████████████████████████████████████| 9600/9600 [1:47:08<00:00,  1.53it/s][INFO|trainer.py:1196] 2021-04-08 21:01:12,068 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 6428.2486, 'train_samples_per_second': 1.493, 'epoch': 5.0}   \n",
      "100%|█████████████████████████████████████| 9600/9600 [1:47:08<00:00,  1.49it/s]\n",
      "[INFO|trainer.py:1648] 2021-04-08 21:01:12,641 >> Saving model checkpoint to ./test-convbert-ner\n",
      "[INFO|configuration_utils.py:329] 2021-04-08 21:01:12,642 >> Configuration saved in ./test-convbert-ner/config.json\n",
      "[INFO|modeling_utils.py:831] 2021-04-08 21:01:14,483 >> Model weights saved in ./test-convbert-ner/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2021-04-08 21:01:14,484 >> tokenizer config file saved in ./test-convbert-ner/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2021-04-08 21:01:14,484 >> Special tokens file saved in ./test-convbert-ner/special_tokens_map.json\n",
      "[INFO|trainer_pt_utils.py:722] 2021-04-08 21:01:14,591 >> ***** train metrics *****\n",
      "[INFO|trainer_pt_utils.py:727] 2021-04-08 21:01:14,592 >>   epoch                      =        5.0\n",
      "[INFO|trainer_pt_utils.py:727] 2021-04-08 21:01:14,592 >>   init_mem_cpu_alloc_delta   =     1975MB\n",
      "[INFO|trainer_pt_utils.py:727] 2021-04-08 21:01:14,592 >>   init_mem_cpu_peaked_delta  =      309MB\n",
      "[INFO|trainer_pt_utils.py:727] 2021-04-08 21:01:14,592 >>   init_mem_gpu_alloc_delta   =      420MB\n",
      "[INFO|trainer_pt_utils.py:727] 2021-04-08 21:01:14,592 >>   init_mem_gpu_peaked_delta  =        0MB\n",
      "[INFO|trainer_pt_utils.py:727] 2021-04-08 21:01:14,592 >>   train_mem_cpu_alloc_delta  =      781MB\n",
      "[INFO|trainer_pt_utils.py:727] 2021-04-08 21:01:14,592 >>   train_mem_cpu_peaked_delta =      281MB\n",
      "[INFO|trainer_pt_utils.py:727] 2021-04-08 21:01:14,592 >>   train_mem_gpu_alloc_delta  =     1246MB\n",
      "[INFO|trainer_pt_utils.py:727] 2021-04-08 21:01:14,592 >>   train_mem_gpu_peaked_delta =     5103MB\n",
      "[INFO|trainer_pt_utils.py:727] 2021-04-08 21:01:14,593 >>   train_runtime              = 1:47:08.24\n",
      "[INFO|trainer_pt_utils.py:727] 2021-04-08 21:01:14,593 >>   train_samples              =     122836\n",
      "[INFO|trainer_pt_utils.py:727] 2021-04-08 21:01:14,593 >>   train_samples_per_second   =      1.493\n",
      "04/08/2021 21:01:14 - INFO - __main__ -   *** Evaluate ***\n",
      "[INFO|trainer.py:1865] 2021-04-08 21:01:14,885 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1866] 2021-04-08 21:01:14,886 >>   Num examples = 10000\n",
      "[INFO|trainer.py:1867] 2021-04-08 21:01:14,886 >>   Batch size = 16\n",
      "100%|█████████████████████████████████████████| 625/625 [02:28<00:00,  4.21it/s]\n",
      "[INFO|trainer_pt_utils.py:722] 2021-04-08 21:03:43,759 >> ***** eval metrics *****\n",
      "[INFO|trainer_pt_utils.py:727] 2021-04-08 21:03:43,759 >>   epoch                     =        5.0\n",
      "[INFO|trainer_pt_utils.py:727] 2021-04-08 21:03:43,759 >>   eval_accuracy             =     0.9869\n",
      "[INFO|trainer_pt_utils.py:727] 2021-04-08 21:03:43,759 >>   eval_f1                   =     0.9633\n",
      "[INFO|trainer_pt_utils.py:727] 2021-04-08 21:03:43,759 >>   eval_loss                 =     0.0665\n",
      "[INFO|trainer_pt_utils.py:727] 2021-04-08 21:03:43,759 >>   eval_mem_cpu_alloc_delta  =       32MB\n",
      "[INFO|trainer_pt_utils.py:727] 2021-04-08 21:03:43,759 >>   eval_mem_cpu_peaked_delta =       13MB\n",
      "[INFO|trainer_pt_utils.py:727] 2021-04-08 21:03:43,759 >>   eval_mem_gpu_alloc_delta  =        0MB\n",
      "[INFO|trainer_pt_utils.py:727] 2021-04-08 21:03:43,760 >>   eval_mem_gpu_peaked_delta =       54MB\n",
      "[INFO|trainer_pt_utils.py:727] 2021-04-08 21:03:43,760 >>   eval_precision            =     0.9583\n",
      "[INFO|trainer_pt_utils.py:727] 2021-04-08 21:03:43,760 >>   eval_recall               =     0.9684\n",
      "[INFO|trainer_pt_utils.py:727] 2021-04-08 21:03:43,760 >>   eval_runtime              = 0:02:28.50\n",
      "[INFO|trainer_pt_utils.py:727] 2021-04-08 21:03:43,760 >>   eval_samples              =      10000\n",
      "[INFO|trainer_pt_utils.py:727] 2021-04-08 21:03:43,760 >>   eval_samples_per_second   =     67.336\n"
     ]
    }
   ],
   "source": [
    "!python run_ner.py \\\n",
    "  --model_name_or_path dbmdz/convbert-base-turkish-cased \\\n",
    "  --train_file ./all_train.json \\\n",
    "  --validation_file ./dev.json \\\n",
    "  --test_file ./test.json \\\n",
    "  --output_dir ./test-convbert-ner \\\n",
    "  --num_train_epochs 5 \\\n",
    "  --per_device_train_batch_size 32\\\n",
    "  --do_train \\\n",
    "  --do_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./test-convbert-ner\")\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"./test-convbert-ner/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "pipeline = pipeline('ner', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pipeline(\"Türkiye Cumhuriyeti Devlet Demiryolları, Türk Hava Yolları, Maden Tetkik ve Arama Genel Müdürlüğü, Hıfzıssıhha Enstitüsü, Türkkuşu, Sümerbank, Etibank, Türk Tarih Kurumu, Türk Dil Kurumu ve daha birçok kamu kurumu Atatürk tarafından veya Atatürk'ün desteğiyle kuruldu.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': 'Türkiye',\n",
       "  'score': 0.999670684337616,\n",
       "  'entity': 'LABEL_1',\n",
       "  'index': 1,\n",
       "  'start': 0,\n",
       "  'end': 7},\n",
       " {'word': 'Cumhuriyeti',\n",
       "  'score': 0.9995294809341431,\n",
       "  'entity': 'LABEL_4',\n",
       "  'index': 2,\n",
       "  'start': 8,\n",
       "  'end': 19},\n",
       " {'word': 'Devlet',\n",
       "  'score': 0.999575674533844,\n",
       "  'entity': 'LABEL_4',\n",
       "  'index': 3,\n",
       "  'start': 20,\n",
       "  'end': 26},\n",
       " {'word': 'Demir',\n",
       "  'score': 0.9997301697731018,\n",
       "  'entity': 'LABEL_4',\n",
       "  'index': 4,\n",
       "  'start': 27,\n",
       "  'end': 32},\n",
       " {'word': '##yol',\n",
       "  'score': 0.999679446220398,\n",
       "  'entity': 'LABEL_4',\n",
       "  'index': 5,\n",
       "  'start': 32,\n",
       "  'end': 35},\n",
       " {'word': '##ları',\n",
       "  'score': 0.9996030330657959,\n",
       "  'entity': 'LABEL_4',\n",
       "  'index': 6,\n",
       "  'start': 35,\n",
       "  'end': 39},\n",
       " {'word': ',',\n",
       "  'score': 0.9998310804367065,\n",
       "  'entity': 'LABEL_6',\n",
       "  'index': 7,\n",
       "  'start': 39,\n",
       "  'end': 40},\n",
       " {'word': 'Türk',\n",
       "  'score': 0.9994691610336304,\n",
       "  'entity': 'LABEL_1',\n",
       "  'index': 8,\n",
       "  'start': 41,\n",
       "  'end': 45},\n",
       " {'word': 'Hava',\n",
       "  'score': 0.9997096657752991,\n",
       "  'entity': 'LABEL_4',\n",
       "  'index': 9,\n",
       "  'start': 46,\n",
       "  'end': 50},\n",
       " {'word': 'Yolları',\n",
       "  'score': 0.99970543384552,\n",
       "  'entity': 'LABEL_4',\n",
       "  'index': 10,\n",
       "  'start': 51,\n",
       "  'end': 58},\n",
       " {'word': ',',\n",
       "  'score': 0.9996899366378784,\n",
       "  'entity': 'LABEL_6',\n",
       "  'index': 11,\n",
       "  'start': 58,\n",
       "  'end': 59},\n",
       " {'word': 'Maden',\n",
       "  'score': 0.9994643926620483,\n",
       "  'entity': 'LABEL_1',\n",
       "  'index': 12,\n",
       "  'start': 60,\n",
       "  'end': 65},\n",
       " {'word': 'Te',\n",
       "  'score': 0.9996501207351685,\n",
       "  'entity': 'LABEL_4',\n",
       "  'index': 13,\n",
       "  'start': 66,\n",
       "  'end': 68},\n",
       " {'word': '##t',\n",
       "  'score': 0.9994623064994812,\n",
       "  'entity': 'LABEL_4',\n",
       "  'index': 14,\n",
       "  'start': 68,\n",
       "  'end': 69},\n",
       " {'word': '##ki',\n",
       "  'score': 0.9996057748794556,\n",
       "  'entity': 'LABEL_4',\n",
       "  'index': 15,\n",
       "  'start': 69,\n",
       "  'end': 71},\n",
       " {'word': '##k',\n",
       "  'score': 0.9996896982192993,\n",
       "  'entity': 'LABEL_4',\n",
       "  'index': 16,\n",
       "  'start': 71,\n",
       "  'end': 72},\n",
       " {'word': 've',\n",
       "  'score': 0.9996824860572815,\n",
       "  'entity': 'LABEL_4',\n",
       "  'index': 17,\n",
       "  'start': 73,\n",
       "  'end': 75},\n",
       " {'word': 'Arama',\n",
       "  'score': 0.9996957778930664,\n",
       "  'entity': 'LABEL_4',\n",
       "  'index': 18,\n",
       "  'start': 76,\n",
       "  'end': 81},\n",
       " {'word': 'Genel',\n",
       "  'score': 0.999695897102356,\n",
       "  'entity': 'LABEL_4',\n",
       "  'index': 19,\n",
       "  'start': 82,\n",
       "  'end': 87},\n",
       " {'word': 'Müdürlüğü',\n",
       "  'score': 0.9996863603591919,\n",
       "  'entity': 'LABEL_4',\n",
       "  'index': 20,\n",
       "  'start': 88,\n",
       "  'end': 97},\n",
       " {'word': ',',\n",
       "  'score': 0.9997289776802063,\n",
       "  'entity': 'LABEL_6',\n",
       "  'index': 21,\n",
       "  'start': 97,\n",
       "  'end': 98},\n",
       " {'word': 'H',\n",
       "  'score': 0.9994601011276245,\n",
       "  'entity': 'LABEL_1',\n",
       "  'index': 22,\n",
       "  'start': 99,\n",
       "  'end': 100},\n",
       " {'word': '##ıf',\n",
       "  'score': 0.9992415904998779,\n",
       "  'entity': 'LABEL_4',\n",
       "  'index': 23,\n",
       "  'start': 100,\n",
       "  'end': 102},\n",
       " {'word': '##zı',\n",
       "  'score': 0.9995495080947876,\n",
       "  'entity': 'LABEL_4',\n",
       "  'index': 24,\n",
       "  'start': 102,\n",
       "  'end': 104},\n",
       " {'word': '##ss',\n",
       "  'score': 0.999654233455658,\n",
       "  'entity': 'LABEL_4',\n",
       "  'index': 25,\n",
       "  'start': 104,\n",
       "  'end': 106},\n",
       " {'word': '##ıh',\n",
       "  'score': 0.9996976256370544,\n",
       "  'entity': 'LABEL_4',\n",
       "  'index': 26,\n",
       "  'start': 106,\n",
       "  'end': 108},\n",
       " {'word': '##ha',\n",
       "  'score': 0.999631404876709,\n",
       "  'entity': 'LABEL_4',\n",
       "  'index': 27,\n",
       "  'start': 108,\n",
       "  'end': 110},\n",
       " {'word': 'Enstitüsü',\n",
       "  'score': 0.9997132420539856,\n",
       "  'entity': 'LABEL_4',\n",
       "  'index': 28,\n",
       "  'start': 111,\n",
       "  'end': 120},\n",
       " {'word': ',',\n",
       "  'score': 0.9997370839118958,\n",
       "  'entity': 'LABEL_6',\n",
       "  'index': 29,\n",
       "  'start': 120,\n",
       "  'end': 121},\n",
       " {'word': 'Türk',\n",
       "  'score': 0.9985498785972595,\n",
       "  'entity': 'LABEL_1',\n",
       "  'index': 30,\n",
       "  'start': 122,\n",
       "  'end': 126},\n",
       " {'word': '##kuş',\n",
       "  'score': 0.9996809959411621,\n",
       "  'entity': 'LABEL_4',\n",
       "  'index': 31,\n",
       "  'start': 126,\n",
       "  'end': 129},\n",
       " {'word': '##u',\n",
       "  'score': 0.9994404315948486,\n",
       "  'entity': 'LABEL_4',\n",
       "  'index': 32,\n",
       "  'start': 129,\n",
       "  'end': 130},\n",
       " {'word': ',',\n",
       "  'score': 0.9997485876083374,\n",
       "  'entity': 'LABEL_6',\n",
       "  'index': 33,\n",
       "  'start': 130,\n",
       "  'end': 131},\n",
       " {'word': 'Sümer',\n",
       "  'score': 0.9993919134140015,\n",
       "  'entity': 'LABEL_1',\n",
       "  'index': 34,\n",
       "  'start': 132,\n",
       "  'end': 137},\n",
       " {'word': '##bank',\n",
       "  'score': 0.9996956586837769,\n",
       "  'entity': 'LABEL_4',\n",
       "  'index': 35,\n",
       "  'start': 137,\n",
       "  'end': 141},\n",
       " {'word': ',',\n",
       "  'score': 0.9997003674507141,\n",
       "  'entity': 'LABEL_6',\n",
       "  'index': 36,\n",
       "  'start': 141,\n",
       "  'end': 142},\n",
       " {'word': 'Eti',\n",
       "  'score': 0.9987291097640991,\n",
       "  'entity': 'LABEL_1',\n",
       "  'index': 37,\n",
       "  'start': 143,\n",
       "  'end': 146},\n",
       " {'word': '##bank',\n",
       "  'score': 0.9996320009231567,\n",
       "  'entity': 'LABEL_4',\n",
       "  'index': 38,\n",
       "  'start': 146,\n",
       "  'end': 150},\n",
       " {'word': ',',\n",
       "  'score': 0.9998533725738525,\n",
       "  'entity': 'LABEL_6',\n",
       "  'index': 39,\n",
       "  'start': 150,\n",
       "  'end': 151},\n",
       " {'word': 'Türk',\n",
       "  'score': 0.999595582485199,\n",
       "  'entity': 'LABEL_1',\n",
       "  'index': 40,\n",
       "  'start': 152,\n",
       "  'end': 156},\n",
       " {'word': 'Tarih',\n",
       "  'score': 0.9997222423553467,\n",
       "  'entity': 'LABEL_4',\n",
       "  'index': 41,\n",
       "  'start': 157,\n",
       "  'end': 162},\n",
       " {'word': 'Kurumu',\n",
       "  'score': 0.9997204542160034,\n",
       "  'entity': 'LABEL_4',\n",
       "  'index': 42,\n",
       "  'start': 163,\n",
       "  'end': 169},\n",
       " {'word': ',',\n",
       "  'score': 0.9998653531074524,\n",
       "  'entity': 'LABEL_6',\n",
       "  'index': 43,\n",
       "  'start': 169,\n",
       "  'end': 170},\n",
       " {'word': 'Türk',\n",
       "  'score': 0.9996249079704285,\n",
       "  'entity': 'LABEL_1',\n",
       "  'index': 44,\n",
       "  'start': 171,\n",
       "  'end': 175},\n",
       " {'word': 'Dil',\n",
       "  'score': 0.9997066855430603,\n",
       "  'entity': 'LABEL_4',\n",
       "  'index': 45,\n",
       "  'start': 176,\n",
       "  'end': 179},\n",
       " {'word': 'Kurumu',\n",
       "  'score': 0.9997006058692932,\n",
       "  'entity': 'LABEL_4',\n",
       "  'index': 46,\n",
       "  'start': 180,\n",
       "  'end': 186},\n",
       " {'word': 've',\n",
       "  'score': 0.9999473094940186,\n",
       "  'entity': 'LABEL_6',\n",
       "  'index': 47,\n",
       "  'start': 187,\n",
       "  'end': 189},\n",
       " {'word': 'daha',\n",
       "  'score': 0.999946117401123,\n",
       "  'entity': 'LABEL_6',\n",
       "  'index': 48,\n",
       "  'start': 190,\n",
       "  'end': 194},\n",
       " {'word': 'birçok',\n",
       "  'score': 0.9999536871910095,\n",
       "  'entity': 'LABEL_6',\n",
       "  'index': 49,\n",
       "  'start': 195,\n",
       "  'end': 201},\n",
       " {'word': 'kamu',\n",
       "  'score': 0.9999086260795593,\n",
       "  'entity': 'LABEL_6',\n",
       "  'index': 50,\n",
       "  'start': 202,\n",
       "  'end': 206},\n",
       " {'word': 'kurumu',\n",
       "  'score': 0.9999398589134216,\n",
       "  'entity': 'LABEL_6',\n",
       "  'index': 51,\n",
       "  'start': 207,\n",
       "  'end': 213},\n",
       " {'word': 'Atatürk',\n",
       "  'score': 0.9767419099807739,\n",
       "  'entity': 'LABEL_2',\n",
       "  'index': 52,\n",
       "  'start': 214,\n",
       "  'end': 221},\n",
       " {'word': 'tarafından',\n",
       "  'score': 0.9998859763145447,\n",
       "  'entity': 'LABEL_6',\n",
       "  'index': 53,\n",
       "  'start': 222,\n",
       "  'end': 232},\n",
       " {'word': 'veya',\n",
       "  'score': 0.9999432563781738,\n",
       "  'entity': 'LABEL_6',\n",
       "  'index': 54,\n",
       "  'start': 233,\n",
       "  'end': 237},\n",
       " {'word': 'Atatürk',\n",
       "  'score': 0.8980334997177124,\n",
       "  'entity': 'LABEL_2',\n",
       "  'index': 55,\n",
       "  'start': 238,\n",
       "  'end': 245},\n",
       " {'word': \"'\",\n",
       "  'score': 0.9994426369667053,\n",
       "  'entity': 'LABEL_6',\n",
       "  'index': 56,\n",
       "  'start': 245,\n",
       "  'end': 246},\n",
       " {'word': 'ün',\n",
       "  'score': 0.9941859245300293,\n",
       "  'entity': 'LABEL_6',\n",
       "  'index': 57,\n",
       "  'start': 246,\n",
       "  'end': 248},\n",
       " {'word': 'desteğiyle',\n",
       "  'score': 0.9997920393943787,\n",
       "  'entity': 'LABEL_6',\n",
       "  'index': 58,\n",
       "  'start': 249,\n",
       "  'end': 259},\n",
       " {'word': 'kuruldu',\n",
       "  'score': 0.9999597072601318,\n",
       "  'entity': 'LABEL_6',\n",
       "  'index': 59,\n",
       "  'start': 260,\n",
       "  'end': 267},\n",
       " {'word': '.',\n",
       "  'score': 0.999950647354126,\n",
       "  'entity': 'LABEL_6',\n",
       "  'index': 60,\n",
       "  'start': 267,\n",
       "  'end': 268}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83 LABEL_6\n",
      "9 LABEL_6\n",
      "' LABEL_6\n",
      "da LABEL_6\n",
      "Koca LABEL_6\n",
      "cık LABEL_6\n",
      "' LABEL_6\n",
      "ta LABEL_6\n",
      "doğduğu LABEL_6\n",
      "sanı LABEL_6\n",
      "lan LABEL_6\n",
      "babası LABEL_6\n",
      "Ali LABEL_6\n",
      "Rıza LABEL_6\n",
      "Efendi LABEL_6\n",
      ", LABEL_6\n",
      "as LABEL_0\n",
      "len LABEL_0\n",
      "Manastır LABEL_6\n",
      "' LABEL_6\n",
      "a LABEL_6\n",
      "bağlı LABEL_6\n",
      "Deb LABEL_6\n",
      "re LABEL_6\n",
      "- LABEL_6\n",
      "i LABEL_6\n",
      "B LABEL_6\n",
      "âlâ LABEL_6\n",
      "' LABEL_6\n",
      "dan LABEL_6\n",
      "dır LABEL_2\n",
      ". LABEL_2\n",
      "Fal LABEL_2\n",
      "ih LABEL_2\n",
      "Rıf LABEL_5\n",
      "kı LABEL_5\n",
      "Atay LABEL_5\n",
      ", LABEL_5\n",
      "Va LABEL_2\n",
      "mı LABEL_5\n",
      "k LABEL_5\n",
      "Volkan LABEL_5\n",
      ", LABEL_5\n",
      "Nor LABEL_2\n",
      "man LABEL_5\n",
      "It LABEL_5\n",
      "z LABEL_5\n",
      "ko LABEL_5\n",
      "wit LABEL_5\n",
      "z LABEL_5\n",
      ", LABEL_5\n",
      "Müj LABEL_2\n",
      "g LABEL_5\n",
      "ân LABEL_5\n",
      "C LABEL_5\n",
      "un LABEL_5\n",
      "bur LABEL_5\n",
      ", LABEL_5\n",
      "Num LABEL_2\n",
      "an LABEL_5\n",
      "Kartal LABEL_5\n",
      "ve LABEL_2\n",
      "Hasan LABEL_2\n",
      "İzzet LABEL_5\n",
      "tin LABEL_5\n",
      "Din LABEL_5\n",
      "amo LABEL_5\n",
      "' LABEL_6\n",
      "ya LABEL_6\n",
      "göre LABEL_6\n",
      ", LABEL_6\n",
      "babasının LABEL_6\n",
      "ailesi LABEL_6\n",
      "14 LABEL_6\n",
      "- LABEL_6\n",
      "15 LABEL_6\n",
      ". LABEL_6\n",
      "yüzyılda LABEL_6\n",
      "Anadolu LABEL_6\n",
      "' LABEL_6\n",
      "dan LABEL_6\n",
      "bölgeye LABEL_6\n",
      "göç LABEL_6\n",
      "etmiş LABEL_6\n",
      "olan LABEL_0\n",
      "Koca LABEL_6\n",
      "cık LABEL_6\n",
      "Yörük LABEL_6\n",
      "lerinden LABEL_6\n",
      "dir LABEL_6\n",
      ". LABEL_6\n",
      "Bazı LABEL_6\n",
      "yabancı LABEL_6\n",
      "kaynaklara LABEL_6\n",
      "göre LABEL_6\n",
      "ise LABEL_6\n",
      "babasının LABEL_6\n",
      "ailesi LABEL_6\n",
      "nde LABEL_6\n",
      "Arnavut LABEL_6\n",
      "veya LABEL_6\n",
      "S LABEL_6\n",
      "lav LABEL_6\n",
      "kökenli LABEL_6\n",
      "Müslümanlar LABEL_6\n",
      "olabilir LABEL_6\n",
      ". LABEL_6\n",
      "Ali LABEL_6\n",
      "Rıza LABEL_6\n",
      "Bey LABEL_6\n",
      "öncelikle LABEL_6\n",
      "dini LABEL_6\n",
      "vakıf LABEL_6\n",
      "ları LABEL_6\n",
      "denet LABEL_6\n",
      "leyen LABEL_6\n",
      "bir LABEL_6\n",
      "memur LABEL_6\n",
      "olarak LABEL_6\n",
      "çalışmış LABEL_6\n",
      ", LABEL_1\n",
      "93 LABEL_1\n",
      "Har LABEL_6\n",
      "bi LABEL_6\n",
      "öncesinde LABEL_6\n",
      "187 LABEL_6\n",
      "6 LABEL_6\n",
      "- LABEL_6\n",
      "77 LABEL_6\n",
      "yıllarında LABEL_6\n",
      "yerel LABEL_6\n",
      "birlik LABEL_6\n",
      "lerde LABEL_6\n",
      "gönüllü LABEL_6\n",
      "te LABEL_6\n",
      "ğ LABEL_6\n",
      "men LABEL_6\n",
      "olarak LABEL_6\n",
      "görev LABEL_6\n",
      "yapmıştır LABEL_6\n",
      ". LABEL_6\n",
      "Zübey LABEL_6\n",
      "de LABEL_6\n",
      "Hanım LABEL_6\n",
      "ile LABEL_5\n",
      "evlendi LABEL_0\n",
      "k LABEL_0\n",
      "ten LABEL_0\n",
      "sonra LABEL_0\n",
      "Selanik LABEL_6\n",
      "' LABEL_0\n",
      "te LABEL_6\n",
      "gümrük LABEL_6\n",
      "memur LABEL_6\n",
      "luğu LABEL_6\n",
      "ve LABEL_6\n",
      "kere LABEL_6\n",
      "ste LABEL_6\n",
      "ticareti LABEL_6\n",
      "yle LABEL_6\n",
      "meşgul LABEL_6\n",
      "oldu LABEL_6\n",
      ". LABEL_6\n"
     ]
    }
   ],
   "source": [
    "for i in res:\n",
    "    if i[\"word\"].startswith(\"##\"):\n",
    "        word = i[\"word\"][2:]\n",
    "    else:\n",
    "        word = i[\"word\"]\n",
    "    print(word, i[\"entity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
